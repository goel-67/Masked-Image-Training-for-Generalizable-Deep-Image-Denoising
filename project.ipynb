{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64 samples per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "att mask only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "loading model from model_zoo/input_mask_80_90.pth\n",
      "testset/McM/HR\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Testing 0 1                    - PSNR: 26.07 dB; SSIM: 0.7950; PSNR_Y: 29.60 dB; SSIM_Y: 0.8804; LPIPS: 0.0824\n",
      "Testing 1 10                   - PSNR: 30.87 dB; SSIM: 0.7837; PSNR_Y: 33.97 dB; SSIM_Y: 0.9291; LPIPS: 0.0707\n",
      "Testing 2 11                   - PSNR: 31.53 dB; SSIM: 0.7438; PSNR_Y: 34.52 dB; SSIM_Y: 0.9231; LPIPS: 0.0951\n",
      "Testing 3 12                   - PSNR: 31.58 dB; SSIM: 0.8491; PSNR_Y: 34.80 dB; SSIM_Y: 0.9278; LPIPS: 0.1323\n",
      "Testing 4 13                   - PSNR: 33.44 dB; SSIM: 0.8531; PSNR_Y: 36.89 dB; SSIM_Y: 0.9134; LPIPS: 0.1878\n",
      "Testing 5 14                   - PSNR: 31.67 dB; SSIM: 0.7854; PSNR_Y: 35.13 dB; SSIM_Y: 0.9206; LPIPS: 0.1084\n",
      "Testing 6 15                   - PSNR: 31.25 dB; SSIM: 0.6888; PSNR_Y: 34.62 dB; SSIM_Y: 0.9216; LPIPS: 0.0804\n",
      "Testing 7 16                   - PSNR: 28.47 dB; SSIM: 0.7828; PSNR_Y: 30.80 dB; SSIM_Y: 0.9025; LPIPS: 0.0620\n",
      "Testing 8 17                   - PSNR: 28.11 dB; SSIM: 0.7827; PSNR_Y: 33.10 dB; SSIM_Y: 0.9105; LPIPS: 0.0875\n",
      "Testing 9 18                   - PSNR: 29.20 dB; SSIM: 0.8134; PSNR_Y: 31.03 dB; SSIM_Y: 0.8871; LPIPS: 0.1087\n",
      "Testing 10 2                    - PSNR: 29.31 dB; SSIM: 0.7734; PSNR_Y: 32.51 dB; SSIM_Y: 0.9119; LPIPS: 0.1016\n",
      "Testing 11 3                    - PSNR: 28.82 dB; SSIM: 0.8625; PSNR_Y: 31.47 dB; SSIM_Y: 0.9203; LPIPS: 0.0627\n",
      "Testing 12 4                    - PSNR: 28.33 dB; SSIM: 0.8899; PSNR_Y: 30.87 dB; SSIM_Y: 0.9211; LPIPS: 0.1212\n",
      "Testing 13 5                    - PSNR: 29.16 dB; SSIM: 0.8196; PSNR_Y: 32.32 dB; SSIM_Y: 0.8907; LPIPS: 0.0996\n",
      "Testing 14 6                    - PSNR: 30.56 dB; SSIM: 0.8334; PSNR_Y: 33.86 dB; SSIM_Y: 0.9135; LPIPS: 0.0922\n",
      "Testing 15 7                    - PSNR: 29.92 dB; SSIM: 0.8058; PSNR_Y: 31.91 dB; SSIM_Y: 0.8861; LPIPS: 0.1689\n",
      "Testing 16 8                    - PSNR: 31.75 dB; SSIM: 0.6786; PSNR_Y: 33.57 dB; SSIM_Y: 0.9440; LPIPS: 0.0815\n",
      "Testing 17 9                    - PSNR: 30.98 dB; SSIM: 0.8174; PSNR_Y: 34.10 dB; SSIM_Y: 0.9209; LPIPS: 0.1001\n",
      "\n",
      "results/input_mask_80_90/McM_poisson_20_att \n",
      "-- Average PSNR/SSIM(RGB): 30.06 dB; 0.7977\n",
      "-- Average PSNR_Y/SSIM_Y/LPIPS: 33.06/0.9125/0.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_test_swinir.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model = torch.load(args.model_path)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --model_path model_zoo/input_mask_80_90.pth --name input_mask_80_90/McM_poisson_20_att --opt model_zoo/input_mask_80_90.json --folder_gt testset/McM/HR --folder_lq testset/McM/McM_poisson_20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input mask only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "loading model from model_zoo/input_mask_80_90.pth\n",
      "testset/McM/HR\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Testing 0 1                    - PSNR: 15.08 dB; SSIM: 0.6354; PSNR_Y: 16.88 dB; SSIM_Y: 0.7292; LPIPS: 0.2580\n",
      "Testing 1 10                   - PSNR: 9.86 dB; SSIM: 0.4227; PSNR_Y: 11.41 dB; SSIM_Y: 0.5578; LPIPS: 0.3692\n",
      "Testing 2 11                   - PSNR: 9.98 dB; SSIM: 0.4675; PSNR_Y: 11.49 dB; SSIM_Y: 0.6398; LPIPS: 0.3655\n",
      "Testing 3 12                   - PSNR: 13.90 dB; SSIM: 0.6081; PSNR_Y: 15.50 dB; SSIM_Y: 0.7046; LPIPS: 0.3588\n",
      "Testing 4 13                   - PSNR: 20.46 dB; SSIM: 0.5966; PSNR_Y: 23.12 dB; SSIM_Y: 0.7162; LPIPS: 0.3887\n",
      "Testing 5 14                   - PSNR: 12.59 dB; SSIM: 0.5246; PSNR_Y: 14.03 dB; SSIM_Y: 0.6707; LPIPS: 0.3669\n",
      "Testing 6 15                   - PSNR: 8.46 dB; SSIM: 0.3948; PSNR_Y: 9.92 dB; SSIM_Y: 0.5655; LPIPS: 0.3898\n",
      "Testing 7 16                   - PSNR: 20.20 dB; SSIM: 0.5901; PSNR_Y: 21.98 dB; SSIM_Y: 0.6886; LPIPS: 0.2545\n",
      "Testing 8 17                   - PSNR: 9.72 dB; SSIM: 0.3776; PSNR_Y: 11.64 dB; SSIM_Y: 0.4552; LPIPS: 0.3730\n",
      "Testing 9 18                   - PSNR: 16.20 dB; SSIM: 0.6414; PSNR_Y: 18.21 dB; SSIM_Y: 0.7462; LPIPS: 0.3060\n",
      "Testing 10 2                    - PSNR: 11.00 dB; SSIM: 0.4714; PSNR_Y: 12.43 dB; SSIM_Y: 0.5977; LPIPS: 0.3526\n",
      "Testing 11 3                    - PSNR: 20.00 dB; SSIM: 0.6947; PSNR_Y: 22.94 dB; SSIM_Y: 0.7854; LPIPS: 0.2950\n",
      "Testing 12 4                    - PSNR: 22.72 dB; SSIM: 0.7129; PSNR_Y: 25.29 dB; SSIM_Y: 0.7758; LPIPS: 0.3497\n",
      "Testing 13 5                    - PSNR: 17.78 dB; SSIM: 0.6330; PSNR_Y: 19.70 dB; SSIM_Y: 0.7346; LPIPS: 0.2777\n",
      "Testing 14 6                    - PSNR: 16.53 dB; SSIM: 0.6404; PSNR_Y: 18.35 dB; SSIM_Y: 0.7434; LPIPS: 0.2386\n",
      "Testing 15 7                    - PSNR: 15.75 dB; SSIM: 0.6161; PSNR_Y: 17.62 dB; SSIM_Y: 0.7381; LPIPS: 0.4349\n",
      "Testing 16 8                    - PSNR: 16.69 dB; SSIM: 0.5729; PSNR_Y: 18.36 dB; SSIM_Y: 0.8407; LPIPS: 0.2679\n",
      "Testing 17 9                    - PSNR: 13.76 dB; SSIM: 0.5869; PSNR_Y: 15.52 dB; SSIM_Y: 0.7108; LPIPS: 0.3297\n",
      "\n",
      "results/input_mask_80_90/McM_poisson_20_inp \n",
      "-- Average PSNR/SSIM(RGB): 15.04 dB; 0.5659\n",
      "-- Average PSNR_Y/SSIM_Y/LPIPS: 16.91/0.6889/0.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_test_swinir.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model = torch.load(args.model_path)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --model_path model_zoo/input_mask_80_90.pth --name input_mask_80_90/McM_poisson_20_inp --opt model_zoo/input_mask_80_90.json --folder_gt testset/McM/HR --folder_lq testset/McM/McM_poisson_20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "loading model from model_zoo/input_mask_80_90.pth\n",
      "testset/McM/HR\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Testing 0 1                    - PSNR: 26.08 dB; SSIM: 0.7957; PSNR_Y: 29.59 dB; SSIM_Y: 0.8797; LPIPS: 0.0872\n",
      "Testing 1 10                   - PSNR: 31.10 dB; SSIM: 0.7844; PSNR_Y: 34.16 dB; SSIM_Y: 0.9313; LPIPS: 0.0702\n",
      "Testing 2 11                   - PSNR: 31.52 dB; SSIM: 0.7412; PSNR_Y: 34.35 dB; SSIM_Y: 0.9241; LPIPS: 0.0958\n",
      "Testing 3 12                   - PSNR: 31.71 dB; SSIM: 0.8530; PSNR_Y: 35.00 dB; SSIM_Y: 0.9315; LPIPS: 0.1306\n",
      "Testing 4 13                   - PSNR: 33.64 dB; SSIM: 0.8591; PSNR_Y: 37.17 dB; SSIM_Y: 0.9184; LPIPS: 0.1873\n",
      "Testing 5 14                   - PSNR: 31.89 dB; SSIM: 0.7931; PSNR_Y: 35.29 dB; SSIM_Y: 0.9218; LPIPS: 0.1127\n",
      "Testing 6 15                   - PSNR: 31.47 dB; SSIM: 0.6884; PSNR_Y: 34.80 dB; SSIM_Y: 0.9233; LPIPS: 0.0821\n",
      "Testing 7 16                   - PSNR: 28.53 dB; SSIM: 0.7810; PSNR_Y: 30.84 dB; SSIM_Y: 0.9015; LPIPS: 0.0633\n",
      "Testing 8 17                   - PSNR: 28.35 dB; SSIM: 0.7902; PSNR_Y: 33.17 dB; SSIM_Y: 0.9112; LPIPS: 0.0876\n",
      "Testing 9 18                   - PSNR: 29.15 dB; SSIM: 0.8137; PSNR_Y: 30.93 dB; SSIM_Y: 0.8877; LPIPS: 0.1097\n",
      "Testing 10 2                    - PSNR: 29.46 dB; SSIM: 0.7733; PSNR_Y: 32.52 dB; SSIM_Y: 0.9118; LPIPS: 0.1009\n",
      "Testing 11 3                    - PSNR: 28.84 dB; SSIM: 0.8624; PSNR_Y: 31.44 dB; SSIM_Y: 0.9216; LPIPS: 0.0632\n",
      "Testing 12 4                    - PSNR: 28.02 dB; SSIM: 0.8847; PSNR_Y: 30.40 dB; SSIM_Y: 0.9165; LPIPS: 0.1245\n",
      "Testing 13 5                    - PSNR: 29.23 dB; SSIM: 0.8213; PSNR_Y: 32.34 dB; SSIM_Y: 0.8919; LPIPS: 0.0977\n",
      "Testing 14 6                    - PSNR: 30.73 dB; SSIM: 0.8350; PSNR_Y: 33.99 dB; SSIM_Y: 0.9152; LPIPS: 0.0879\n",
      "Testing 15 7                    - PSNR: 29.90 dB; SSIM: 0.8029; PSNR_Y: 31.82 dB; SSIM_Y: 0.8855; LPIPS: 0.1721\n",
      "Testing 16 8                    - PSNR: 31.70 dB; SSIM: 0.6718; PSNR_Y: 33.52 dB; SSIM_Y: 0.9439; LPIPS: 0.0830\n",
      "Testing 17 9                    - PSNR: 31.02 dB; SSIM: 0.8183; PSNR_Y: 34.01 dB; SSIM_Y: 0.9224; LPIPS: 0.1017\n",
      "\n",
      "results/input_mask_80_90/McM_poisson_20_both \n",
      "-- Average PSNR/SSIM(RGB): 30.13 dB; 0.7983\n",
      "-- Average PSNR_Y/SSIM_Y/LPIPS: 33.07/0.9133/0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_test_swinir.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model = torch.load(args.model_path)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --model_path model_zoo/input_mask_80_90.pth --name input_mask_80_90/McM_poisson_20_both --opt model_zoo/input_mask_80_90.json --folder_gt testset/McM/HR --folder_lq testset/McM/McM_poisson_20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing different mask ratios "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "loading model from model_zoo/input_mask_80_90.pth\n",
      "testset/McM/HR\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Testing 0 1                    - PSNR: 25.95 dB; SSIM: 0.7897; PSNR_Y: 29.38 dB; SSIM_Y: 0.8752; LPIPS: 0.1013\n",
      "Testing 1 10                   - PSNR: 31.03 dB; SSIM: 0.7765; PSNR_Y: 34.09 dB; SSIM_Y: 0.9329; LPIPS: 0.0740\n",
      "Testing 2 11                   - PSNR: 30.98 dB; SSIM: 0.7194; PSNR_Y: 33.50 dB; SSIM_Y: 0.9225; LPIPS: 0.1015\n",
      "Testing 3 12                   - PSNR: 31.56 dB; SSIM: 0.8529; PSNR_Y: 34.98 dB; SSIM_Y: 0.9348; LPIPS: 0.1359\n",
      "Testing 4 13                   - PSNR: 33.80 dB; SSIM: 0.8680; PSNR_Y: 37.53 dB; SSIM_Y: 0.9260; LPIPS: 0.1944\n",
      "Testing 5 14                   - PSNR: 31.76 dB; SSIM: 0.7941; PSNR_Y: 35.28 dB; SSIM_Y: 0.9223; LPIPS: 0.1209\n",
      "Testing 6 15                   - PSNR: 31.33 dB; SSIM: 0.6762; PSNR_Y: 34.56 dB; SSIM_Y: 0.9242; LPIPS: 0.0861\n",
      "Testing 7 16                   - PSNR: 28.29 dB; SSIM: 0.7713; PSNR_Y: 30.46 dB; SSIM_Y: 0.8968; LPIPS: 0.0701\n",
      "Testing 8 17                   - PSNR: 28.27 dB; SSIM: 0.7870; PSNR_Y: 32.88 dB; SSIM_Y: 0.9087; LPIPS: 0.0941\n",
      "Testing 9 18                   - PSNR: 28.72 dB; SSIM: 0.8064; PSNR_Y: 30.38 dB; SSIM_Y: 0.8848; LPIPS: 0.1212\n",
      "Testing 10 2                    - PSNR: 29.33 dB; SSIM: 0.7595; PSNR_Y: 32.21 dB; SSIM_Y: 0.9095; LPIPS: 0.1049\n",
      "Testing 11 3                    - PSNR: 28.71 dB; SSIM: 0.8572; PSNR_Y: 31.26 dB; SSIM_Y: 0.9210; LPIPS: 0.0684\n",
      "Testing 12 4                    - PSNR: 27.30 dB; SSIM: 0.8758; PSNR_Y: 29.41 dB; SSIM_Y: 0.9093; LPIPS: 0.1319\n",
      "Testing 13 5                    - PSNR: 29.22 dB; SSIM: 0.8220; PSNR_Y: 32.35 dB; SSIM_Y: 0.8934; LPIPS: 0.1002\n",
      "Testing 14 6                    - PSNR: 30.80 dB; SSIM: 0.8323; PSNR_Y: 34.06 dB; SSIM_Y: 0.9159; LPIPS: 0.0886\n",
      "Testing 15 7                    - PSNR: 29.77 dB; SSIM: 0.7953; PSNR_Y: 31.64 dB; SSIM_Y: 0.8838; LPIPS: 0.1796\n",
      "Testing 16 8                    - PSNR: 31.45 dB; SSIM: 0.6543; PSNR_Y: 33.24 dB; SSIM_Y: 0.9422; LPIPS: 0.0850\n",
      "Testing 17 9                    - PSNR: 30.80 dB; SSIM: 0.8124; PSNR_Y: 33.59 dB; SSIM_Y: 0.9235; LPIPS: 0.1046\n",
      "\n",
      "results/input_mask_80_90/McM_poisson_20_65 \n",
      "-- Average PSNR/SSIM(RGB): 29.95 dB; 0.7917\n",
      "-- Average PSNR_Y/SSIM_Y/LPIPS: 32.82/0.9126/0.1090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_test_swinir.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model = torch.load(args.model_path)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --model_path model_zoo/input_mask_80_90.pth --name input_mask_80_90/McM_poisson_20_65 --opt model_zoo/input_mask_80_90.json --folder_gt testset/McM/HR --folder_lq testset/McM/McM_poisson_20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "loading model from model_zoo/input_mask_80_90.pth\n",
      "testset/McM/HR\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Testing 0 1                    - PSNR: 26.08 dB; SSIM: 0.7960; PSNR_Y: 29.59 dB; SSIM_Y: 0.8800; LPIPS: 0.0876\n",
      "Testing 1 10                   - PSNR: 31.08 dB; SSIM: 0.7842; PSNR_Y: 34.13 dB; SSIM_Y: 0.9314; LPIPS: 0.0699\n",
      "Testing 2 11                   - PSNR: 31.53 dB; SSIM: 0.7417; PSNR_Y: 34.35 dB; SSIM_Y: 0.9240; LPIPS: 0.0961\n",
      "Testing 3 12                   - PSNR: 31.70 dB; SSIM: 0.8528; PSNR_Y: 34.99 dB; SSIM_Y: 0.9315; LPIPS: 0.1304\n",
      "Testing 4 13                   - PSNR: 33.64 dB; SSIM: 0.8592; PSNR_Y: 37.16 dB; SSIM_Y: 0.9185; LPIPS: 0.1878\n",
      "Testing 5 14                   - PSNR: 31.88 dB; SSIM: 0.7922; PSNR_Y: 35.28 dB; SSIM_Y: 0.9216; LPIPS: 0.1119\n",
      "Testing 6 15                   - PSNR: 31.47 dB; SSIM: 0.6874; PSNR_Y: 34.79 dB; SSIM_Y: 0.9233; LPIPS: 0.0816\n",
      "Testing 7 16                   - PSNR: 28.52 dB; SSIM: 0.7800; PSNR_Y: 30.83 dB; SSIM_Y: 0.9014; LPIPS: 0.0634\n",
      "Testing 8 17                   - PSNR: 28.36 dB; SSIM: 0.7904; PSNR_Y: 33.17 dB; SSIM_Y: 0.9114; LPIPS: 0.0880\n",
      "Testing 9 18                   - PSNR: 29.14 dB; SSIM: 0.8134; PSNR_Y: 30.92 dB; SSIM_Y: 0.8875; LPIPS: 0.1103\n",
      "Testing 10 2                    - PSNR: 29.45 dB; SSIM: 0.7742; PSNR_Y: 32.52 dB; SSIM_Y: 0.9120; LPIPS: 0.1002\n",
      "Testing 11 3                    - PSNR: 28.82 dB; SSIM: 0.8620; PSNR_Y: 31.43 dB; SSIM_Y: 0.9212; LPIPS: 0.0643\n",
      "Testing 12 4                    - PSNR: 28.02 dB; SSIM: 0.8847; PSNR_Y: 30.41 dB; SSIM_Y: 0.9165; LPIPS: 0.1253\n",
      "Testing 13 5                    - PSNR: 29.22 dB; SSIM: 0.8208; PSNR_Y: 32.33 dB; SSIM_Y: 0.8915; LPIPS: 0.0984\n",
      "Testing 14 6                    - PSNR: 30.72 dB; SSIM: 0.8349; PSNR_Y: 33.98 dB; SSIM_Y: 0.9151; LPIPS: 0.0893\n",
      "Testing 15 7                    - PSNR: 29.90 dB; SSIM: 0.8031; PSNR_Y: 31.82 dB; SSIM_Y: 0.8856; LPIPS: 0.1722\n",
      "Testing 16 8                    - PSNR: 31.71 dB; SSIM: 0.6721; PSNR_Y: 33.52 dB; SSIM_Y: 0.9438; LPIPS: 0.0828\n",
      "Testing 17 9                    - PSNR: 31.05 dB; SSIM: 0.8186; PSNR_Y: 34.05 dB; SSIM_Y: 0.9228; LPIPS: 0.1000\n",
      "\n",
      "results/input_mask_80_90/McM_poisson_20_75 \n",
      "-- Average PSNR/SSIM(RGB): 30.13 dB; 0.7982\n",
      "-- Average PSNR_Y/SSIM_Y/LPIPS: 33.07/0.9133/0.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_test_swinir.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model = torch.load(args.model_path)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --model_path model_zoo/input_mask_80_90.pth --name input_mask_80_90/McM_poisson_20_75 --opt model_zoo/input_mask_80_90.json --folder_gt testset/McM/HR --folder_lq testset/McM/McM_poisson_20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "loading model from model_zoo/input_mask_80_90.pth\n",
      "testset/McM/HR\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Testing 0 1                    - PSNR: 25.98 dB; SSIM: 0.7918; PSNR_Y: 29.52 dB; SSIM_Y: 0.8800; LPIPS: 0.0801\n",
      "Testing 1 10                   - PSNR: 30.36 dB; SSIM: 0.7784; PSNR_Y: 33.61 dB; SSIM_Y: 0.9255; LPIPS: 0.0740\n",
      "Testing 2 11                   - PSNR: 31.14 dB; SSIM: 0.7383; PSNR_Y: 34.29 dB; SSIM_Y: 0.9212; LPIPS: 0.0951\n",
      "Testing 3 12                   - PSNR: 31.23 dB; SSIM: 0.8423; PSNR_Y: 34.37 dB; SSIM_Y: 0.9221; LPIPS: 0.1366\n",
      "Testing 4 13                   - PSNR: 33.10 dB; SSIM: 0.8438; PSNR_Y: 36.48 dB; SSIM_Y: 0.9059; LPIPS: 0.1930\n",
      "Testing 5 14                   - PSNR: 31.08 dB; SSIM: 0.7755; PSNR_Y: 34.80 dB; SSIM_Y: 0.9184; LPIPS: 0.1092\n",
      "Testing 6 15                   - PSNR: 30.73 dB; SSIM: 0.6850; PSNR_Y: 34.13 dB; SSIM_Y: 0.9187; LPIPS: 0.0807\n",
      "Testing 7 16                   - PSNR: 28.17 dB; SSIM: 0.7841; PSNR_Y: 30.41 dB; SSIM_Y: 0.9025; LPIPS: 0.0627\n",
      "Testing 8 17                   - PSNR: 27.64 dB; SSIM: 0.7730; PSNR_Y: 32.81 dB; SSIM_Y: 0.9075; LPIPS: 0.0912\n",
      "Testing 9 18                   - PSNR: 29.16 dB; SSIM: 0.8128; PSNR_Y: 31.08 dB; SSIM_Y: 0.8866; LPIPS: 0.1068\n",
      "Testing 10 2                    - PSNR: 28.90 dB; SSIM: 0.7640; PSNR_Y: 32.33 dB; SSIM_Y: 0.9104; LPIPS: 0.1053\n",
      "Testing 11 3                    - PSNR: 28.76 dB; SSIM: 0.8609; PSNR_Y: 31.45 dB; SSIM_Y: 0.9183; LPIPS: 0.0637\n",
      "Testing 12 4                    - PSNR: 28.58 dB; SSIM: 0.8936; PSNR_Y: 31.29 dB; SSIM_Y: 0.9245; LPIPS: 0.1196\n",
      "Testing 13 5                    - PSNR: 29.04 dB; SSIM: 0.8172; PSNR_Y: 32.29 dB; SSIM_Y: 0.8893; LPIPS: 0.1024\n",
      "Testing 14 6                    - PSNR: 30.31 dB; SSIM: 0.8294; PSNR_Y: 33.68 dB; SSIM_Y: 0.9114; LPIPS: 0.0957\n",
      "Testing 15 7                    - PSNR: 29.87 dB; SSIM: 0.8073; PSNR_Y: 31.97 dB; SSIM_Y: 0.8860; LPIPS: 0.1673\n",
      "Testing 16 8                    - PSNR: 31.72 dB; SSIM: 0.6835; PSNR_Y: 33.54 dB; SSIM_Y: 0.9433; LPIPS: 0.0824\n",
      "Testing 17 9                    - PSNR: 30.71 dB; SSIM: 0.8104; PSNR_Y: 34.01 dB; SSIM_Y: 0.9172; LPIPS: 0.1041\n",
      "\n",
      "results/input_mask_80_90/McM_poisson_20_85 \n",
      "-- Average PSNR/SSIM(RGB): 29.80 dB; 0.7940\n",
      "-- Average PSNR_Y/SSIM_Y/LPIPS: 32.89/0.9105/0.1039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_test_swinir.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model = torch.load(args.model_path)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --model_path model_zoo/input_mask_80_90.pth --name input_mask_80_90/McM_poisson_20_85 --opt model_zoo/input_mask_80_90.json --folder_gt testset/McM/HR --folder_lq testset/McM/McM_poisson_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laksh\\AppData\\Local\\Temp\\ipykernel_26484\\2480988375.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('model_zoo/input_mask_80_90.pth')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_zoo/input_mask_80_90.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()  \u001b[38;5;66;03m# Set the model to evaluation mode if only testing\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Modify the model, for example, by changing the output layer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, new_num_classes)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models  # or import your specific model if it's custom\n",
    "\n",
    "# Create the model architecture (e.g., ResNet50)\n",
    "model = models.resnet50()  # Replace with your model type if different\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('model_zoo/input_mask_80_90.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Modify the model, for example, by changing the output layer\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, new_num_classes)\n",
    "\n",
    "# Optionally fine-tune the model\n",
    "model.train()  # Set back to training mode for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n",
      "LogHandlers setup!\n",
      "Random seed: 1\n",
      "len(self.paths_H): 68\n",
      "Dataset [DatasetMaskedDenoising - train_dataset] is created.\n",
      "Get L/H for image-to-image mapping. Both \"paths_L\" and \"paths_H\" are needed.\n",
      "Dataset [DatasetPlain - test_dataset] is created.\n",
      "Pass this initialization! Initialization was done during network definition!\n",
      "Pass this initialization! Initialization was done during network definition!\n",
      "Training model [ModelPlain] is created.\n",
      "Copying model for E ...\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-11-10 00:50:14.961 :   task: input_80_90\n",
      "  model: plain\n",
      "  gpu_ids: [0]\n",
      "  dist: False\n",
      "  scale: 1\n",
      "  n_channels: 3\n",
      "  path:[\n",
      "    root: masked_denoising\n",
      "    pretrained_netG: None\n",
      "    pretrained_netE: None\n",
      "    task: masked_denoising\\input_80_90\n",
      "    log: masked_denoising\\input_80_90\n",
      "    options: masked_denoising\\input_80_90\\options\n",
      "    models: masked_denoising\\input_80_90\\models\n",
      "    images: masked_denoising\\input_80_90\\images\n",
      "    pretrained_optimizerG: None\n",
      "  ]\n",
      "  datasets:[\n",
      "    train:[\n",
      "      name: train_dataset\n",
      "      dataset_type: masked_denoising\n",
      "      dataroot_H: trainsets/train\n",
      "      dataroot_L: None\n",
      "      H_size: 64\n",
      "      lq_patchsize: 64\n",
      "      dataloader_shuffle: True\n",
      "      dataloader_num_workers: 16\n",
      "      dataloader_batch_size: 64\n",
      "      noise_level: 15\n",
      "      if_mask: True\n",
      "      mask1: 80\n",
      "      mask2: 90\n",
      "      phase: train\n",
      "      scale: 1\n",
      "      n_channels: 3\n",
      "    ]\n",
      "    test:[\n",
      "      name: test_dataset\n",
      "      dataset_type: plain\n",
      "      dataroot_H: testset/McM/HR\n",
      "      dataroot_L: testset/McM/McM_poisson_20\n",
      "      phase: test\n",
      "      scale: 1\n",
      "      n_channels: 3\n",
      "    ]\n",
      "  ]\n",
      "  netG:[\n",
      "    net_type: swinir\n",
      "    upscale: 1\n",
      "    in_chans: 3\n",
      "    img_size: 64\n",
      "    window_size: 8\n",
      "    img_range: 1.0\n",
      "    depths: [6, 6, 6, 6]\n",
      "    embed_dim: 60\n",
      "    num_heads: [6, 6, 6, 6]\n",
      "    mlp_ratio: 2\n",
      "    upsampler: None\n",
      "    resi_connection: 3conv\n",
      "    init_type: default\n",
      "    talking_heads: False\n",
      "    attn_fn: softmax\n",
      "    head_scale: False\n",
      "    on_attn: False\n",
      "    use_mask: True\n",
      "    mask_ratio1: 75\n",
      "    mask_ratio2: 75\n",
      "    mask_is_diff: False\n",
      "    type: stand\n",
      "    scale: 1\n",
      "  ]\n",
      "  train:[\n",
      "    manual_seed: 1\n",
      "    G_lossfn_type: l1\n",
      "    G_lossfn_weight: 1.0\n",
      "    E_decay: 0.999\n",
      "    G_optimizer_type: adam\n",
      "    G_optimizer_lr: 0.0001\n",
      "    G_optimizer_wd: 0\n",
      "    G_optimizer_clipgrad: None\n",
      "    G_optimizer_reuse: True\n",
      "    G_scheduler_type: MultiStepLR\n",
      "    G_scheduler_milestones: []\n",
      "    G_scheduler_gamma: 0.5\n",
      "    G_regularizer_orthstep: None\n",
      "    G_regularizer_clipstep: None\n",
      "    G_param_strict: True\n",
      "    E_param_strict: True\n",
      "    checkpoint_test: 5000\n",
      "    checkpoint_save: 5000\n",
      "    checkpoint_print: 100\n",
      "    save_image: ['img_043_x1', 'img_021_x1', 'img_024_x1', 'img_031_x1', 'img_041_x1', 'img_032_x1']\n",
      "    initial_mask_ratio: 90\n",
      "    final_mask_ratio: 50\n",
      "    mask_schedule_steps: 100000\n",
      "    F_feature_layer: 34\n",
      "    F_weights: 1.0\n",
      "    F_lossfn_type: l1\n",
      "    F_use_input_norm: True\n",
      "    F_use_range_norm: False\n",
      "    G_optimizer_betas: [0.9, 0.999]\n",
      "    G_scheduler_restart_weights: 1\n",
      "  ]\n",
      "  opt_path: options/masked_denoising/input_mask_80_90.json\n",
      "  is_train: True\n",
      "  merge_bn: False\n",
      "  merge_bn_startpoint: -1\n",
      "  find_unused_parameters: True\n",
      "  use_static_graph: False\n",
      "  num_gpu: 1\n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "\n",
      "24-11-10 00:50:15.275 : Number of train images: 68, iters: 2\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "24-11-10 00:50:16.566 : \n",
      "Networks name: SwinIR\n",
      "Params number: 824118\n",
      "Net structure:\n",
      "SwinIR(\n",
      "  (conv_first): Conv2d(3, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (patch_unembed): PatchUnEmbed()\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.004)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.009)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.017)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.022)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(60, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (2): Conv2d(15, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (4): Conv2d(15, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (1): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.030)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.035)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.043)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.048)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(60, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (2): Conv2d(15, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (4): Conv2d(15, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (2): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.057)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.061)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.070)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.074)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(60, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (2): Conv2d(15, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (4): Conv2d(15, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (3): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.083)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.087)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.096)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.100)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(60, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (2): Conv2d(15, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (4): Conv2d(15, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "  (conv_after_body): Sequential(\n",
      "    (0): Conv2d(60, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(15, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(15, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv_last): Conv2d(60, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\n",
      "24-11-10 00:50:16.714 : \n",
      " |  mean  |  min   |  max   |  std   || shape               \n",
      " |  0.028 | -0.564 |  0.576 |  0.327 | torch.Size([60, 3, 1, 1]) || conv_first.weight\n",
      " |  0.013 | -0.519 |  0.568 |  0.365 | torch.Size([60]) || conv_first.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || patch_embed.norm.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || patch_embed.norm.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.bias\n",
      " |  0.001 | -0.062 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.0.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.qkv.bias\n",
      " |  0.000 | -0.069 |  0.075 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.0.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.bias\n",
      " | -0.000 | -0.067 |  0.085 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.0.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.0.mlp.fc1.bias\n",
      " | -0.000 | -0.083 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.0.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.bias\n",
      " |  0.000 | -0.081 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.068 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.qkv.bias\n",
      " | -0.001 | -0.081 |  0.060 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.bias\n",
      " | -0.000 | -0.074 |  0.085 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.1.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.1.mlp.fc1.bias\n",
      " | -0.000 | -0.074 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.1.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.bias\n",
      " |  0.000 | -0.066 |  0.070 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index\n",
      " |  0.000 | -0.086 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.2.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.qkv.bias\n",
      " | -0.001 | -0.064 |  0.067 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.bias\n",
      " |  0.000 | -0.072 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.2.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.2.mlp.fc1.bias\n",
      " |  0.000 | -0.073 |  0.076 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.2.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.bias\n",
      " | -0.000 | -0.083 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index\n",
      " |  0.000 | -0.089 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.3.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.qkv.bias\n",
      " |  0.000 | -0.070 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.3.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.bias\n",
      " | -0.000 | -0.082 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.3.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.3.mlp.fc1.bias\n",
      " |  0.000 | -0.077 |  0.078 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.3.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm1.bias\n",
      " |  0.000 | -0.073 |  0.069 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.084 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.4.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.qkv.bias\n",
      " | -0.000 | -0.076 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.4.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm2.bias\n",
      " | -0.000 | -0.075 |  0.082 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.4.mlp.fc1.bias\n",
      " |  0.000 | -0.074 |  0.069 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.4.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm1.bias\n",
      " | -0.000 | -0.063 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index\n",
      " |  0.000 | -0.073 |  0.079 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.5.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.qkv.bias\n",
      " |  0.000 | -0.068 |  0.075 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.5.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.5.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm2.bias\n",
      " |  0.000 | -0.073 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.5.mlp.fc1.bias\n",
      " |  0.000 | -0.072 |  0.068 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.5.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.5.mlp.fc2.bias\n",
      " |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([15, 60, 3, 3]) || layers.0.conv.0.weight\n",
      " | -0.003 | -0.036 |  0.040 |  0.027 | torch.Size([15]) || layers.0.conv.0.bias\n",
      " |  0.014 | -0.258 |  0.256 |  0.154 | torch.Size([15, 15, 1, 1]) || layers.0.conv.2.weight\n",
      " |  0.098 | -0.208 |  0.244 |  0.122 | torch.Size([15]) || layers.0.conv.2.bias\n",
      " |  0.000 | -0.086 |  0.086 |  0.049 | torch.Size([60, 15, 3, 3]) || layers.0.conv.4.weight\n",
      " |  0.002 | -0.085 |  0.085 |  0.057 | torch.Size([60]) || layers.0.conv.4.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.bias\n",
      " |  0.000 | -0.074 |  0.060 |  0.019 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.081 |  0.069 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.0.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.qkv.bias\n",
      " |  0.001 | -0.069 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.0.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.bias\n",
      " |  0.000 | -0.073 |  0.096 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.0.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.0.mlp.fc1.bias\n",
      " | -0.000 | -0.074 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.0.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.bias\n",
      " |  0.000 | -0.061 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index\n",
      " |  0.000 | -0.089 |  0.069 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.qkv.bias\n",
      " | -0.000 | -0.073 |  0.089 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.bias\n",
      " | -0.000 | -0.074 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.1.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.1.mlp.fc1.bias\n",
      " |  0.000 | -0.073 |  0.071 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.1.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.bias\n",
      " |  0.001 | -0.073 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index\n",
      " | -0.000 | -0.077 |  0.099 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.2.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.qkv.bias\n",
      " |  0.000 | -0.075 |  0.067 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.bias\n",
      " |  0.000 | -0.076 |  0.067 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.2.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.2.mlp.fc1.bias\n",
      " |  0.000 | -0.079 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.2.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.bias\n",
      " | -0.000 | -0.057 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.071 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.3.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.qkv.bias\n",
      " |  0.000 | -0.069 |  0.069 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.3.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.bias\n",
      " | -0.000 | -0.071 |  0.069 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.3.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.3.mlp.fc1.bias\n",
      " | -0.000 | -0.074 |  0.077 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.3.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm1.bias\n",
      " |  0.000 | -0.069 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index\n",
      " | -0.000 | -0.071 |  0.070 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.4.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.qkv.bias\n",
      " | -0.000 | -0.087 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.4.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm2.bias\n",
      " | -0.000 | -0.075 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.4.mlp.fc1.bias\n",
      " |  0.000 | -0.070 |  0.072 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.4.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm1.bias\n",
      " |  0.001 | -0.072 |  0.077 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index\n",
      " |  0.000 | -0.080 |  0.072 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.5.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.qkv.bias\n",
      " | -0.000 | -0.081 |  0.076 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.5.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.5.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm2.bias\n",
      " | -0.000 | -0.077 |  0.083 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.5.mlp.fc1.bias\n",
      " | -0.000 | -0.072 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.5.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.5.mlp.fc2.bias\n",
      " | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([15, 60, 3, 3]) || layers.1.conv.0.weight\n",
      " |  0.002 | -0.030 |  0.042 |  0.023 | torch.Size([15]) || layers.1.conv.0.bias\n",
      " |  0.004 | -0.253 |  0.254 |  0.139 | torch.Size([15, 15, 1, 1]) || layers.1.conv.2.weight\n",
      " |  0.022 | -0.216 |  0.211 |  0.148 | torch.Size([15]) || layers.1.conv.2.bias\n",
      " |  0.001 | -0.086 |  0.086 |  0.050 | torch.Size([60, 15, 3, 3]) || layers.1.conv.4.weight\n",
      " | -0.004 | -0.085 |  0.085 |  0.048 | torch.Size([60]) || layers.1.conv.4.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.bias\n",
      " |  0.001 | -0.069 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.077 |  0.069 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.0.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.qkv.bias\n",
      " |  0.000 | -0.074 |  0.076 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.0.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.bias\n",
      " | -0.000 | -0.095 |  0.072 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.0.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.0.mlp.fc1.bias\n",
      " |  0.000 | -0.071 |  0.070 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.0.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.bias\n",
      " |  0.000 | -0.057 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.071 |  0.078 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.qkv.bias\n",
      " |  0.000 | -0.087 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.bias\n",
      " | -0.000 | -0.070 |  0.071 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.1.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.1.mlp.fc1.bias\n",
      " | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.1.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.bias\n",
      " |  0.001 | -0.080 |  0.099 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index\n",
      " | -0.000 | -0.071 |  0.095 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.2.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.qkv.bias\n",
      " |  0.000 | -0.079 |  0.079 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.bias\n",
      " |  0.000 | -0.070 |  0.075 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.2.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.2.mlp.fc1.bias\n",
      " |  0.000 | -0.072 |  0.071 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.2.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.bias\n",
      " |  0.000 | -0.053 |  0.070 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index\n",
      " | -0.000 | -0.095 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.3.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.qkv.bias\n",
      " | -0.000 | -0.070 |  0.069 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.3.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.bias\n",
      " | -0.000 | -0.071 |  0.077 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.3.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.3.mlp.fc1.bias\n",
      " |  0.000 | -0.076 |  0.069 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.3.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm1.bias\n",
      " | -0.000 | -0.069 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index\n",
      " | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.4.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.qkv.bias\n",
      " | -0.000 | -0.070 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.4.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm2.bias\n",
      " |  0.000 | -0.079 |  0.076 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.4.mlp.fc1.bias\n",
      " | -0.000 | -0.076 |  0.071 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.4.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm1.bias\n",
      " | -0.000 | -0.070 |  0.079 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index\n",
      " |  0.000 | -0.076 |  0.084 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.5.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.qkv.bias\n",
      " | -0.000 | -0.085 |  0.069 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.5.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.5.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm2.bias\n",
      " | -0.000 | -0.080 |  0.070 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.5.mlp.fc1.bias\n",
      " | -0.000 | -0.073 |  0.075 |  0.019 | torch.Size([60, 120]) || layers.2.residual_group.blocks.5.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.5.mlp.fc2.bias\n",
      " |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([15, 60, 3, 3]) || layers.2.conv.0.weight\n",
      " | -0.015 | -0.041 |  0.034 |  0.020 | torch.Size([15]) || layers.2.conv.0.bias\n",
      " | -0.011 | -0.254 |  0.256 |  0.152 | torch.Size([15, 15, 1, 1]) || layers.2.conv.2.weight\n",
      " | -0.022 | -0.207 |  0.225 |  0.164 | torch.Size([15]) || layers.2.conv.2.bias\n",
      " |  0.000 | -0.086 |  0.086 |  0.050 | torch.Size([60, 15, 3, 3]) || layers.2.conv.4.weight\n",
      " | -0.009 | -0.084 |  0.084 |  0.054 | torch.Size([60]) || layers.2.conv.4.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.bias\n",
      " |  0.001 | -0.071 |  0.068 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.077 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.0.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.qkv.bias\n",
      " |  0.000 | -0.069 |  0.072 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.0.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.bias\n",
      " |  0.000 | -0.080 |  0.071 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.0.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.0.mlp.fc1.bias\n",
      " | -0.000 | -0.072 |  0.081 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.0.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.bias\n",
      " | -0.001 | -0.065 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.069 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.qkv.bias\n",
      " |  0.000 | -0.064 |  0.072 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.bias\n",
      " |  0.000 | -0.073 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.1.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.1.mlp.fc1.bias\n",
      " |  0.000 | -0.065 |  0.079 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.1.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.bias\n",
      " |  0.002 | -0.062 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index\n",
      " | -0.000 | -0.066 |  0.084 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.2.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.qkv.bias\n",
      " | -0.000 | -0.076 |  0.069 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.bias\n",
      " |  0.000 | -0.101 |  0.071 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.2.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.2.mlp.fc1.bias\n",
      " | -0.000 | -0.079 |  0.078 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.2.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.bias\n",
      " | -0.000 | -0.067 |  0.085 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.072 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.3.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.qkv.bias\n",
      " | -0.001 | -0.072 |  0.075 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.3.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.bias\n",
      " |  0.000 | -0.066 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.3.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.3.mlp.fc1.bias\n",
      " |  0.000 | -0.068 |  0.080 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.3.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm1.bias\n",
      " | -0.001 | -0.066 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index\n",
      " | -0.000 | -0.076 |  0.085 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.4.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.qkv.bias\n",
      " |  0.000 | -0.068 |  0.070 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.4.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm2.bias\n",
      " |  0.000 | -0.071 |  0.067 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.4.mlp.fc1.bias\n",
      " |  0.000 | -0.081 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.4.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm1.bias\n",
      " | -0.000 | -0.062 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index\n",
      " | -0.000 | -0.070 |  0.075 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.5.attn.qkv.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.qkv.bias\n",
      " | -0.000 | -0.067 |  0.077 |  0.021 | torch.Size([60, 60]) || layers.3.residual_group.blocks.5.attn.proj.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.5.attn.proj.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm2.bias\n",
      " |  0.000 | -0.078 |  0.076 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.5.mlp.fc1.bias\n",
      " | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.5.mlp.fc2.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.5.mlp.fc2.bias\n",
      " | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([15, 60, 3, 3]) || layers.3.conv.0.weight\n",
      " | -0.010 | -0.037 |  0.028 |  0.022 | torch.Size([15]) || layers.3.conv.0.bias\n",
      " | -0.013 | -0.257 |  0.255 |  0.151 | torch.Size([15, 15, 1, 1]) || layers.3.conv.2.weight\n",
      " |  0.024 | -0.218 |  0.249 |  0.158 | torch.Size([15]) || layers.3.conv.2.bias\n",
      " | -0.000 | -0.086 |  0.086 |  0.049 | torch.Size([60, 15, 3, 3]) || layers.3.conv.4.weight\n",
      " |  0.002 | -0.083 |  0.086 |  0.055 | torch.Size([60]) || layers.3.conv.4.bias\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || norm.weight\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || norm.bias\n",
      " |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([15, 60, 3, 3]) || conv_after_body.0.weight\n",
      " |  0.007 | -0.038 |  0.041 |  0.025 | torch.Size([15]) || conv_after_body.0.bias\n",
      " | -0.004 | -0.257 |  0.256 |  0.144 | torch.Size([15, 15, 1, 1]) || conv_after_body.2.weight\n",
      " | -0.049 | -0.244 |  0.252 |  0.153 | torch.Size([15]) || conv_after_body.2.bias\n",
      " | -0.000 | -0.086 |  0.086 |  0.050 | torch.Size([60, 15, 3, 3]) || conv_after_body.4.weight\n",
      " |  0.009 | -0.086 |  0.086 |  0.050 | torch.Size([60]) || conv_after_body.4.bias\n",
      " |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([3, 60, 3, 3]) || conv_last.weight\n",
      " |  0.010 | -0.032 |  0.034 |  0.037 | torch.Size([3]) || conv_last.bias\n",
      "\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_train_psnr.py\", line 364, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\main_train_psnr.py\", line 243, in main\n",
      "    model.optimize_parameters(current_step)\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\model_plain.py\", line 175, in optimize_parameters\n",
      "    self.netG_forward()\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\model_plain.py\", line 166, in netG_forward\n",
      "    self.E = self.netG(self.L)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\", line 184, in forward\n",
      "    return self.module(*inputs[0], **module_kwargs[0])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\network_swinir.py\", line 1022, in forward\n",
      "    res = self.conv_after_body(self.forward_features(x_first))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\network_swinir.py\", line 983, in forward_features\n",
      "    x = layer(x, x_size)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\network_swinir.py\", line 646, in forward\n",
      "    return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\network_swinir.py\", line 541, in forward\n",
      "    x = blk(x, x_size)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\network_swinir.py\", line 395, in forward\n",
      "    attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\laksh\\Downloads\\MaskedDenoising-main (1)\\MaskedDenoising-main\\models\\network_swinir.py\", line 217, in forward\n",
      "    attn = (q @ k.transpose(-2, -1))\n",
      "            ~~^~~~~~~~~~~~~~~~~~~~~\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 242.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python main_train_psnr.py --opt options/masked_denoising/input_mask_80_90.json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
